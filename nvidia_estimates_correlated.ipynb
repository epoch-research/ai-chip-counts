{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import squigglepy as sq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from squigglepy.numbers import K, M, B\n",
    "\n",
    "sq.set_seed(42)\n",
    "np.random.seed(42)\n",
    "np.seterr(invalid='raise')  # Warn on operations involving NaN\n",
    "N_SAMPLES = 5000\n",
    "\n",
    "from chip_estimates_utils import (\n",
    "    estimate_chip_sales,\n",
    "    estimate_cumulative_chip_sales,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA chip types\n",
    "CHIP_TYPES = ['A100', 'A800', 'H100/H200', 'H800', 'H20', 'B200', 'B300']\n",
    "\n",
    "# Hardware share of compute revenue (vs cloud/software)\n",
    "# This is our main source of revenue uncertainty for NVIDIA\n",
    "HARDWARE_SHARE = sq.to(0.96, 0.99, credibility=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 quarters of data\n",
      "         Compute revenue\n",
      "Quarter                 \n",
      "FY23Q1              2.81\n",
      "FY23Q2              2.86\n",
      "FY23Q3              2.87\n",
      "FY23Q4              2.72\n",
      "FY24Q1              3.35\n",
      "\n",
      "      A100 low  A100 high  Notes  H100 low  H100 high  Geomean  Notes.1  \\\n",
      "Year                                                                      \n",
      "2022     10000      15000    NaN     27000      35000  $30,741      NaN   \n",
      "2023     10000      15000    NaN     27000      35000  $30,741      NaN   \n",
      "2024     10000      15000    NaN     25000      32000  $28,284      NaN   \n",
      "2025     10000      15000    NaN     22000      30000  $25,690      NaN   \n",
      "\n",
      "      B200 low  B200 high Geomean.1  ...  Geomean.2  Notes.3  H20 low  \\\n",
      "Year                                 ...                                \n",
      "2022       NaN        NaN       NaN  ...        NaN      NaN      NaN   \n",
      "2023       NaN        NaN       NaN  ...        NaN      NaN      NaN   \n",
      "2024   33000.0    42000.0   $37,229  ...        NaN      NaN  10000.0   \n",
      "2025   33000.0    42000.0   $37,229  ...    $43,151      NaN  10000.0   \n",
      "\n",
      "     H20 high  Geomean.3  A800 low  A800 high Notes.4  H800 low  H800 high  \n",
      "Year                                                                        \n",
      "2022      NaN        NaN   10000.0    15000.0     NaN       NaN        NaN  \n",
      "2023      NaN        NaN   10000.0    15000.0     NaN   25000.0    35000.0  \n",
      "2024  15000.0    $12,247       NaN        NaN     NaN       NaN        NaN  \n",
      "2025  13000.0    $11,402       NaN        NaN     NaN       NaN        NaN  \n",
      "\n",
      "[4 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load revenue and price data from Google Sheets\n",
    "revenue_df = pd.read_csv(\n",
    "    \"https://docs.google.com/spreadsheets/d/1Yhu87Rw--9tviAuBwg_luL3OFAFkdHdVfli6tN215Xk/export?format=csv&gid=0\"\n",
    ").set_index('Quarter')\n",
    "\n",
    "prices_df = pd.read_csv(\n",
    "    \"https://docs.google.com/spreadsheets/d/1Yhu87Rw--9tviAuBwg_luL3OFAFkdHdVfli6tN215Xk/export?format=csv&gid=1819303346\"\n",
    ").set_index('Year')\n",
    "\n",
    "QUARTERS = revenue_df.index.tolist()\n",
    "\n",
    "print(f\"Loaded {len(QUARTERS)} quarters of data\")\n",
    "print(revenue_df[['Compute revenue']].head())\n",
    "print()\n",
    "print(prices_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "price_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base prices (first year available for each chip):\n",
      "  A100 (2022): $10,000 - $15,000\n",
      "  A800 (2022): $10,000 - $15,000\n",
      "  H100/H200 (2022): $27,000 - $35,000\n",
      "  H800 (2023): $25,000 - $35,000\n",
      "  H20 (2024): $10,000 - $15,000\n",
      "  B200 (2024): $33,000 - $42,000\n",
      "  B300 (2025): $38,000 - $49,000\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# BASE PRICE AND DEFLATION SETUP\n",
    "# ==============================================\n",
    "\n",
    "# Map chip types to their column names in the prices CSV\n",
    "PRICE_COLUMN_MAP = {'H100/H200': 'H100'}\n",
    "\n",
    "# Fallback prices if not found in CSV\n",
    "FALLBACK_PRICES = {\n",
    "    'A100': (10*K, 15*K), 'A800': (10*K, 15*K), 'H100/H200': (20*K, 30*K),\n",
    "    'H800': (20*K, 30*K), 'H20': (10*K, 15*K), 'B200': (33*K, 42*K), 'B300': (33*K, 42*K)\n",
    "}\n",
    "\n",
    "# Build base price distributions (from first available year for each chip)\n",
    "def get_price_dist_for_year(chip, year):\n",
    "    \"\"\"Get price distribution for a chip in a given year.\"\"\"\n",
    "    csv_chip_name = PRICE_COLUMN_MAP.get(chip, chip)\n",
    "    low_col, high_col = f'{csv_chip_name} low', f'{csv_chip_name} high'\n",
    "    \n",
    "    if low_col in prices_df.columns and high_col in prices_df.columns:\n",
    "        if year in prices_df.index:\n",
    "            low = prices_df.loc[year, low_col]\n",
    "            high = prices_df.loc[year, high_col]\n",
    "            if pd.notna(low) and pd.notna(high):\n",
    "                return sq.to(low, high, credibility=80)\n",
    "    \n",
    "    return sq.to(*FALLBACK_PRICES.get(chip, (20*K, 30*K)), credibility=80)\n",
    "\n",
    "# Find first year each chip has price data\n",
    "def find_first_year_with_price(chip):\n",
    "    \"\"\"Find the first year with price data for a chip.\"\"\"\n",
    "    csv_chip_name = PRICE_COLUMN_MAP.get(chip, chip)\n",
    "    low_col = f'{csv_chip_name} low'\n",
    "    \n",
    "    if low_col in prices_df.columns:\n",
    "        for year in sorted(prices_df.index):\n",
    "            if pd.notna(prices_df.loc[year, low_col]):\n",
    "                return year\n",
    "    return min(prices_df.index)  # fallback to first year\n",
    "\n",
    "# Build base prices dict\n",
    "BASE_YEAR = {chip: find_first_year_with_price(chip) for chip in CHIP_TYPES}\n",
    "BASE_PRICES = {chip: get_price_dist_for_year(chip, BASE_YEAR[chip]) for chip in CHIP_TYPES}\n",
    "\n",
    "print(\"Base prices (first year available for each chip):\")\n",
    "for chip in CHIP_TYPES:\n",
    "    dist = BASE_PRICES[chip]\n",
    "    print(f\"  {chip} ({BASE_YEAR[chip]}): ${dist.x:,.0f} - ${dist.y:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deflation factors by year (ratio to base year):\n",
      "  A100: {2022: 1.0, 2023: 1.0, 2024: 1.0, 2025: 1.0}\n",
      "  A800: {2022: 1.0, 2023: 1.0, 2024: 1.0, 2025: 1.0}\n",
      "  H100/H200: {2022: 1.0, 2023: 1.0, 2024: 0.92, 2025: 0.836}\n",
      "  H800: {2022: 1.0, 2023: 1.0, 2024: 1.0, 2025: 1.0}\n",
      "  H20: {2022: 1.0, 2023: 1.0, 2024: 1.0, 2025: 0.931}\n",
      "  B200: {2022: 1.0, 2023: 1.0, 2024: 1.0, 2025: 1.0}\n",
      "  B300: {2022: 1.0, 2023: 1.0, 2024: 1.0, 2025: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# DEFLATION FACTORS\n",
    "# ==============================================\n",
    "\n",
    "def get_price_bounds(chip, year):\n",
    "    \"\"\"Get (low, high) price bounds for a chip in a given year, or None if unavailable.\"\"\"\n",
    "    csv_chip_name = PRICE_COLUMN_MAP.get(chip, chip)\n",
    "    low_col, high_col = f'{csv_chip_name} low', f'{csv_chip_name} high'\n",
    "\n",
    "    if low_col in prices_df.columns and high_col in prices_df.columns:\n",
    "        if year in prices_df.index:\n",
    "            low = prices_df.loc[year, low_col]\n",
    "            high = prices_df.loc[year, high_col]\n",
    "            if pd.notna(low) and pd.notna(high):\n",
    "                return (low, high)\n",
    "    return None\n",
    "\n",
    "def get_price_year_for_quarter(quarter):\n",
    "    \"\"\"Get the calendar year to use for pricing a quarter.\"\"\"\n",
    "    start_date = revenue_df.loc[quarter, 'Start Date']\n",
    "    return pd.to_datetime(start_date).year\n",
    "\n",
    "def get_deflation_factor(quarter, chip):\n",
    "    \"\"\"Get deflation factor for a chip in a quarter (ratio of current price to base price).\"\"\"\n",
    "    price_year = get_price_year_for_quarter(quarter)\n",
    "    base_year = BASE_YEAR[chip]\n",
    "\n",
    "    if price_year <= base_year:\n",
    "        return 1.0\n",
    "\n",
    "    base_bounds = get_price_bounds(chip, base_year)\n",
    "    current_bounds = get_price_bounds(chip, price_year)\n",
    "\n",
    "    if base_bounds and current_bounds:\n",
    "        # For lognormal sq.to(low, high), geometric mean = sqrt(low * high)\n",
    "        return np.sqrt((current_bounds[0] * current_bounds[1]) / (base_bounds[0] * base_bounds[1]))\n",
    "    return 1.0\n",
    "\n",
    "# Print deflation factors for reference\n",
    "print(\"Deflation factors by year (ratio to base year):\")\n",
    "years = sorted(prices_df.index)\n",
    "for chip in CHIP_TYPES:\n",
    "    factors = {}\n",
    "    for year in years:\n",
    "        # Find a quarter in this year to test\n",
    "        for q in QUARTERS:\n",
    "            if get_price_year_for_quarter(q) == year:\n",
    "                factors[year] = round(get_deflation_factor(q, chip), 3)\n",
    "                break\n",
    "    if factors:\n",
    "        print(f\"  {chip}: {factors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling_functions",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================\n# SAMPLING FUNCTIONS\n# ==============================================\n\ndef sample_revenue(quarter):\n    \"\"\"Return base revenue for a quarter (no uncertainty applied here).\"\"\"\n    return revenue_df.loc[quarter, 'Compute revenue'] * B\n\ndef sample_shares(quarter):\n    \"\"\"Sample chip shares for a quarter.\"\"\"\n    return {chip: revenue_df.loc[quarter, f'{chip} share'] for chip in CHIP_TYPES}\n\ndef sample_base_price(chip):\n    \"\"\"Sample base price for a chip (from its first available year).\"\"\"\n    return BASE_PRICES[chip] @ 1\n\ndef sample_revenue_uncertainty():\n    \"\"\"Sample hardware share (our main source of revenue uncertainty).\"\"\"\n    return HARDWARE_SHARE @ 1\n\n# Cache price distributions by (chip, year) to avoid recreating them on every sample\nPRICE_DIST_CACHE = {}\n\ndef sample_price(quarter, chip):\n    \"\"\"Sample price for a chip in a quarter (for uncorrelated model).\"\"\"\n    year = get_price_year_for_quarter(quarter)\n    cache_key = (chip, year)\n    if cache_key not in PRICE_DIST_CACHE:\n        PRICE_DIST_CACHE[cache_key] = get_price_dist_for_year(chip, year)\n    return PRICE_DIST_CACHE[cache_key] @ 1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785c4634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Run Monte Carlo simulation to estimate cumulative chip volumes with correlated parameters.\n",
      "\n",
      "    Similar to estimate_chip_sales, but presamples certain parameters to correlate them\n",
      "    across quarters. Use this when estimating cumulative totals where you want price\n",
      "    uncertainty (and optionally revenue multiplier) to compound rather than average out.\n",
      "\n",
      "    Args:\n",
      "        quarters: list of quarter identifiers (e.g., ['Q1_2023', 'Q2_2023', ...])\n",
      "        chip_types: list of chip types (e.g., ['alpha', 'beta', 'gamma', ...])\n",
      "        sample_revenue: fn(quarter) -> float, samples total chip revenue in dollars for a quarter\n",
      "        sample_shares: fn(quarter) -> dict, samples {chip: share} for a quarter (should sum to 1)\n",
      "        sample_base_price: fn(chip) -> float, samples the BASE price for a chip type\n",
      "            (i.e., the price when the chip was first introduced). Called once per chip;\n",
      "            subsequent quarters use this base price scaled by get_deflation_factor.\n",
      "        get_deflation_factor: fn(quarter, chip) -> float, returns price multiplier for a\n",
      "            quarter relative to the base price. Should return 1.0 for the chip's first\n",
      "            quarter and <1.0 for later quarters as prices decline. If None, no deflation.\n",
      "        sample_revenue_uncertainty: fn() -> float, samples a multiplier for revenue uncertainty.\n",
      "            Sampled once and applied to all quarters. Use this to model correlated uncertainty\n",
      "            in revenue estimates (e.g., lambda: sq.to(0.9, 1.1) @ 1 if revenue could be 10% off\n",
      "            in either direction, consistently across quarters). If None, no multiplier applied.\n",
      "        n_samples: number of Monte Carlo samples\n",
      "\n",
      "    Returns:\n",
      "        dict of {chip: np.array of cumulative chip counts across all quarters}\n",
      "        Each array has n_samples elements representing the distribution of total chips.\n",
      "\n",
      "    Note on correlations:\n",
      "        - Prices are sampled once per chip type and reused (with deflation) across all quarters.\n",
      "          This means if we sample a \"high price world\", that persists for the entire simulation.\n",
      "        - Revenue uncertainty (if provided) is sampled once and applied to all quarters.\n",
      "        - Revenue and production mix shares are sampled independently each quarter.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(estimate_cumulative_chip_sales.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# RUN CORRELATED SIMULATION\n",
    "# ==============================================\n",
    "\n",
    "cumulative_results = estimate_cumulative_chip_sales(\n",
    "    quarters=QUARTERS,\n",
    "    chip_types=CHIP_TYPES,\n",
    "    sample_revenue=sample_revenue,\n",
    "    sample_shares=sample_shares,\n",
    "    sample_base_price=sample_base_price,\n",
    "    get_deflation_factor=get_deflation_factor,\n",
    "    sample_revenue_uncertainty=sample_revenue_uncertainty,\n",
    "    n_samples=N_SAMPLES\n",
    ")\n",
    "\n",
    "print(\"Simulation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative Nvidia Chip Sales (Correlated Model)\n",
      "Version                p5          p50          p95\n",
      "---------------------------------------------------\n",
      "A100              687,284      896,350    1,155,578\n",
      "A800               92,971      120,764      156,666\n",
      "H100/H200       3,630,250    4,315,035    5,078,479\n",
      "H800               93,368      116,313      144,810\n",
      "H20             1,148,323    1,497,729    1,923,943\n",
      "B200            1,366,105    1,597,211    1,876,533\n",
      "B300              789,405      936,230    1,109,063\n",
      "---------------------------------------------------\n",
      "TOTAL           8,642,708    9,521,119   10,483,723\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# CUMULATIVE SUMMARY\n",
    "# ==============================================\n",
    "\n",
    "def print_cumulative_summary(cumulative_results, chip_types, title=\"Cumulative Production\"):\n",
    "    \"\"\"Print formatted summary of cumulative chip counts with percentiles.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"{'Version':<12} {'p5':>12} {'p50':>12} {'p95':>12}\")\n",
    "    print(\"-\" * 51)\n",
    "\n",
    "    grand_total = None\n",
    "    for chip in chip_types:\n",
    "        arr = cumulative_results[chip]\n",
    "        if arr.sum() > 0:\n",
    "            if grand_total is None:\n",
    "                grand_total = np.zeros_like(arr)\n",
    "            grand_total += arr\n",
    "            print(f\"{chip:<12} {int(np.percentile(arr, 5)):>12,} {int(np.percentile(arr, 50)):>12,} {int(np.percentile(arr, 95)):>12,}\")\n",
    "\n",
    "    if grand_total is not None:\n",
    "        print(\"-\" * 51)\n",
    "        print(f\"{'TOTAL':<12} {int(np.percentile(grand_total, 5)):>12,} {int(np.percentile(grand_total, 50)):>12,} {int(np.percentile(grand_total, 95)):>12,}\")\n",
    "\n",
    "print_cumulative_summary(cumulative_results, CHIP_TYPES, \"Cumulative Nvidia Chip Sales (Correlated Model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# UNCORRELATED MODEL COMPARISON\n",
    "# ==============================================\n",
    "\n",
    "# Run uncorrelated simulation (price sampled independently each quarter)\n",
    "uncorrelated = estimate_chip_sales(\n",
    "    quarters=QUARTERS,\n",
    "    versions=CHIP_TYPES,\n",
    "    sample_revenue=sample_revenue,\n",
    "    sample_shares=sample_shares,\n",
    "    sample_price=sample_price,\n",
    "    n_samples=2000 #not 10k \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ee202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative Nvidia Chip Sales (uncorrelated model)\n",
      "Version                p5          p50          p95\n",
      "---------------------------------------------------\n",
      "A100              833,622      931,252    1,046,702\n",
      "A800              109,181      125,327      143,816\n",
      "H100/H200       4,192,386    4,440,386    4,705,190\n",
      "H800              104,295      119,984      139,099\n",
      "H20             1,392,168    1,547,536    1,723,135\n",
      "B200            1,519,193    1,647,631    1,786,075\n",
      "B300              853,785      963,290    1,094,977\n",
      "---------------------------------------------------\n",
      "TOTAL           9,442,899    9,789,548   10,159,322\n"
     ]
    }
   ],
   "source": [
    "cumulative_uncorrelated = {chip: np.zeros(2000) for chip in CHIP_TYPES}\n",
    "for quarter in uncorrelated:\n",
    "    for chip in CHIP_TYPES:\n",
    "        cumulative_uncorrelated[chip] += np.array(uncorrelated[quarter][chip])\n",
    "\n",
    "print_cumulative_summary(cumulative_uncorrelated, CHIP_TYPES, \"Cumulative Nvidia Chip Sales (uncorrelated model)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Plotly)",
   "language": "python",
   "name": "plotly_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}